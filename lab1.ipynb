{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "U5VWPI9kl7DG",
        "outputId": "19458f10-4c80-4780-f059-09226e2a88cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "U5VWPI9kl7DG",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "import re\n",
        "import string\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "FhelTJosmOLa",
        "outputId": "1f6444d3-3d2d-4513-d6e3-04eb118bcda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "FhelTJosmOLa",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('d.csv', on_bad_lines='skip', delimiter=\",\")\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "nEha_sksmS4a"
      },
      "id": "nEha_sksmS4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "data['review'] = data['review'].str.lower()\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "0Dptt_mImW5O"
      },
      "id": "0Dptt_mImW5O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove HTML tags\n",
        "def remove_html_tag(text):\n",
        "    pattern = re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "data['review'] = data['review'].apply(remove_html_tag)\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "HFEYSpFumaN6"
      },
      "id": "HFEYSpFumaN6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs\n",
        "def remove_urls(text):\n",
        "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "data['review'] = data['review'].apply(remove_urls)\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "FQ9BK4Nemcim"
      },
      "id": "FQ9BK4Nemcim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove punctuation (excluding full stop)\n",
        "exclude = '\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
        "def remove_punctuation_optimized(text):\n",
        "    return text.translate(str.maketrans('', '', exclude))\n",
        "\n",
        "data['review'] = data['review'].apply(remove_punctuation_optimized)\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "T1OhVrYDmfYa"
      },
      "id": "T1OhVrYDmfYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove emojis\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002700-\\U000027BF\"  # dingbats\n",
        "        u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
        "        \"]+\",\n",
        "        flags=re.UNICODE\n",
        "    )\n",
        "    return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "data['review'] = data['review'].apply(remove_emoji)\n",
        "display(data.head(10))"
      ],
      "metadata": {
        "id": "wjprtonhmj1w"
      },
      "id": "wjprtonhmj1w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the entire column\n",
        "data['tok_review'] = data['review'].apply(word_tokenize)\n",
        "\n",
        "# Display the first 10 rows with the tokenized reviews\n",
        "display(data[['review', 'tok_review']].head(10))"
      ],
      "metadata": {
        "id": "TLbzevaimzs0"
      },
      "id": "TLbzevaimzs0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming function for tokenized text\n",
        "def stem_words(tokens):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(word) for word in tokens]\n",
        "\n",
        "# Apply stemming to the tokenized reviews\n",
        "data['stemmed_review'] = data['tok_review'].apply(stem_words)\n",
        "\n",
        "# Display the first 10 rows with stemmed reviews\n",
        "display(data[['review', 'tok_review', 'stemmed_review']].head(10))\n"
      ],
      "metadata": {
        "id": "hIn3LXoCnAPE"
      },
      "id": "hIn3LXoCnAPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization function for tokenized text\n",
        "def lemmatize_words(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]\n",
        "\n",
        "# Apply lemmatization to the stemmed reviews\n",
        "data['lemmatized_review'] = data['stemmed_review'].apply(lemmatize_words)\n",
        "\n",
        "# Display the first 10 rows with lemmatized reviews\n",
        "display(data[['review', 'tok_review', 'stemmed_review', 'lemmatized_review']].head(10))\n"
      ],
      "metadata": {
        "id": "inAmAuz1nDte"
      },
      "id": "inAmAuz1nDte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and Import Required Libraries\n",
        "!pip install nltk  # Install NLTK if not already installed\n",
        "import nltk\n",
        "from nltk.metrics import edit_distance\n",
        "\n",
        "# Step 2: Define the Strings\n",
        "s1 = \"cat\"\n",
        "s2 = \"dog\"\n",
        "\n",
        "# Step 3: Compute the Edit Distance\n",
        "distance = edit_distance(s1, s2)\n",
        "\n",
        "# Step 4: Display the Result\n",
        "print(f\"The edit distance between '{s1}' and '{s2}' is: {distance}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vwDZrVfynIkC"
      },
      "id": "vwDZrVfynIkC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}